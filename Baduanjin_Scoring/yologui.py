#è¿™ä¸ªpté¡µé¢æ˜¯ä¸€ä¸ªåˆä»£ç‰ˆæœ¬ï¼Œè¿˜æœªä¼˜åŒ–ï¼Œè¿ç®—é€Ÿåº¦å¯èƒ½ä¼šæ¯”è¾ƒæ…¢ï¼Œè¦æµ‹è¯•çš„è¯å¯ä»¥å»use.pyä¸­

import torch
from PySide6 import QtWidgets, QtCore, QtGui
import cv2, os, time
from threading import Thread
import math
import numpy as np
from matplotlib import pyplot as plt


from prepare.DTW_O_use import dtw_o_distance

# ä¸ç„¶æ¯æ¬¡YOLOå¤„ç†éƒ½ä¼šè¾“å‡ºè°ƒè¯•ä¿¡æ¯
os.environ['YOLO_VERBOSE'] = 'False'
from ultralytics import YOLO 

class MWindow(QtWidgets.QMainWindow):

    def __init__(self):

        super().__init__()
        self.h = 0
        self.keypoints_array = []
        self.list = []
        self.count = 0
        # è®¾ç½®ç•Œé¢
        self.setupUI()

        self.camBtn.clicked.connect(self.startCamera)
        self.videoBtn.clicked.connect(self.startVideoFile)
        self.stopBtn.clicked.connect(self.stop)
        self.similarity.clicked.connect(self.sim)
        self.score.clicked.connect(self.jisuan)
        self.biaoji = []

        # å®šä¹‰å®šæ—¶å™¨ï¼Œç”¨äºæ§åˆ¶æ˜¾ç¤ºæ‘„åƒå¤´è§†é¢‘çš„å¸§ç‡
        self.timer_camera = QtCore.QTimer()
        # å®šæ—¶åˆ°äº†ï¼Œå›è°ƒ self.show_camera
        self.timer_camera.timeout.connect(self.show_camera)

        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
        device = 'cuda' if torch.cuda.is_available() else 'cpu'  # è‡ªåŠ¨é€‰æ‹© GPU æˆ– CPU
        self.model = YOLO(r'D:\KK\ultralytics-main_first\yolov8l-pose.pt').to(device)  # åŠ è½½æ¨¡å‹åˆ°ç›¸åº”çš„è®¾å¤‡
        # self.model = YOLO(r"E:\AIGC\ComfyUI-aki-v1.4\runs\pose\train88\weights\best.pt").to(device)
        # è¦å¤„ç†çš„è§†é¢‘å¸§å›¾ç‰‡é˜Ÿåˆ—ï¼Œç›®å‰å°±æ”¾1å¸§å›¾ç‰‡
        self.frameToAnalyze = []

        # å¯åŠ¨å¤„ç†è§†é¢‘å¸§ç‹¬ç«‹çº¿ç¨‹
        Thread(target=self.frameAnalyzeThreadFunc,daemon=True).start()


        # å®šä¹‰å®šæ—¶å™¨ï¼Œç”¨äºæ§åˆ¶æ˜¾ç¤ºè§†é¢‘æ–‡ä»¶çš„å¸§ç‡
        self.timer_videoFile = QtCore.QTimer()
        # å®šæ—¶åˆ°äº†ï¼Œå›è°ƒ self.show_camera
        self.timer_videoFile.timeout.connect(self.show_videoFile)

        # å½“å‰è¦æ’­æ”¾çš„è§†é¢‘å¸§å·
        self.vframeIdx = 0

        # cv2.VideoCapture å®ä¾‹
        self.cap = None

        self.stopFlag = False

    def setupUI(self):

        self.resize(1200, 800)

        self.setWindowTitle('å…«æ®µé”¦è¯„åˆ†ç³»ç»Ÿ')

        # central Widget
        centralWidget = QtWidgets.QWidget(self)
        self.setCentralWidget(centralWidget)

        # central Widget é‡Œé¢çš„ ä¸» layout
        mainLayout = QtWidgets.QVBoxLayout(centralWidget)

        # ç•Œé¢çš„ä¸ŠåŠéƒ¨åˆ† : å›¾å½¢å±•ç¤ºéƒ¨åˆ†
        topLayout = QtWidgets.QHBoxLayout()
        self.label_ori_video = QtWidgets.QLabel(self)
        self.label_treated = QtWidgets.QLabel(self)
        self.label_ori_video.setFixedSize(520,400)
        self.label_treated.setFixedSize(520,400)
        # self.label_ori_video.setMinimumSize(520,400)
        # self.label_treated.setMinimumSize(520,400)
        self.label_ori_video.setStyleSheet('border:1px solid #D7E2F9;')
        self.label_treated.setStyleSheet('border:1px solid #D7E2F9;')

        topLayout.addWidget(self.label_ori_video)
        topLayout.addWidget(self.label_treated)

        mainLayout.addLayout(topLayout)

        # ç•Œé¢ä¸‹åŠéƒ¨åˆ†ï¼š è¾“å‡ºæ¡† å’Œ æŒ‰é’®
        groupBox = QtWidgets.QGroupBox(self)

        bottomLayout =  QtWidgets.QHBoxLayout(groupBox)
        self.textLog = QtWidgets.QTextBrowser()

        bottomLayout.addWidget(self.textLog)
        mainLayout.addWidget(groupBox)

        btnLayout = QtWidgets.QVBoxLayout()
        self.videoBtn = QtWidgets.QPushButton('ğŸï¸è§†é¢‘æ–‡ä»¶')
        self.camBtn   = QtWidgets.QPushButton('ğŸ“¹æ‘„åƒå¤´')
        self.stopBtn  = QtWidgets.QPushButton('ğŸ›‘åœæ­¢')
        self.score = QtWidgets.QPushButton('è®¡ç®—å¾—åˆ†')
        self.similarity = QtWidgets.QPushButton('å…³é”®å¸§ç›¸ä¼¼åº¦æ˜¾ç¤º')
        btnLayout.addWidget(self.videoBtn)
        btnLayout.addWidget(self.camBtn)
        btnLayout.addWidget(self.stopBtn)
        btnLayout.addWidget(self.score)
        btnLayout.addWidget(self.similarity)
        bottomLayout.addLayout(btnLayout)


    def startCamera(self):

        # å‚è€ƒ https://docs.opencv.org/3.4/dd/d43/tutorial_py_video_display.html

        # åœ¨ windowsä¸ŠæŒ‡å®šä½¿ç”¨ cv2.CAP_DSHOW ä¼šè®©æ‰“å¼€æ‘„åƒå¤´å¿«å¾ˆå¤šï¼Œ 
        # åœ¨ Linux/Macä¸Š æŒ‡å®š V4L, FFMPEG æˆ–è€… GSTREAMER
        self.cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
        if not self.cap.isOpened():
            print("1å·æ‘„åƒå¤´ä¸èƒ½æ‰“å¼€")
            return

        if self.timer_camera.isActive() == False:  # è‹¥å®šæ—¶å™¨æœªå¯åŠ¨
            self.timer_camera.start(30)
            self.stopFlag = False


    def show_camera(self):

        ret, frame = self.cap.read()  # ä»è§†é¢‘æµä¸­è¯»å–
        if not ret:
            return


        # æŠŠè¯»åˆ°çš„å¸§çš„å¤§å°é‡æ–°è®¾ç½® 
        frame = cv2.resize(frame, (520, 400))

        self.setFrameToOriLabel(frame)

    def setFrameToOriLabel(self,frame):

        # è§†é¢‘è‰²å½©è½¬æ¢å›RGBï¼ŒOpenCV images as BGR
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  
        qImage = QtGui.QImage(frame.data, frame.shape[1], frame.shape[0],
                                 QtGui.QImage.Format_RGB888)  # å˜æˆQImageå½¢å¼
        # å¾€æ˜¾ç¤ºè§†é¢‘çš„Labelé‡Œ æ˜¾ç¤ºQImage
        self.label_ori_video.setPixmap(QtGui.QPixmap.fromImage(qImage)) 

        # å¦‚æœå½“å‰æ²¡æœ‰å¤„ç†ä»»åŠ¡
        if not self.frameToAnalyze:
            self.frameToAnalyze.append(frame)

    def frameAnalyzeThreadFunc(self):

        while True:
            if not self.frameToAnalyze:
                # time.sleep(0.01)
                continue

            frame = self.frameToAnalyze.pop(0)

            results = self.model(frame)[0]

            """å¤„ç†æ•°æ®=============================================================="""
            if results:
                for result in results:
                    boxes = result.boxes  # Boxes object for bounding box outputs
                    x_min = boxes.xyxy[0][0]
                    y_min = boxes.xyxy[0][1]
                    masks = result.masks  # Masks object for segmentation masks outputs
                    keypoints = result.keypoints  # Keypoints object for pose outputs
                    if self.h == 0:
                        x1 = ((keypoints.xy[0][5] + keypoints.xy[0][6]) / 2)[0]
                        y1 = ((keypoints.xy[0][5] + keypoints.xy[0][6]) / 2)[1]
                        x2 = ((keypoints.xy[0][11] + keypoints.xy[0][12]) / 2)[0]
                        y2 = ((keypoints.xy[0][11] + keypoints.xy[0][12]) / 2)[1]
                        h = math.sqrt(math.pow((x1 - x2), 2) + math.pow((y1 - y2), 2))

                    frame_keypoints = np.zeros(34)

                    # Normalize keypoints
                    for i, k in enumerate(keypoints.xy[0]):  # Enumerate over the 17 keypoints
                        if k[0] != 0:  # Avoid processing invalid keypoints
                            # Normalize by subtracting the minimum box value and dividing by height
                            normalized_x = (k[0] - x_min) / h
                            normalized_y = (k[1] - y_min) / h
                            frame_keypoints[2 * i] = normalized_x  # Store normalized x
                            frame_keypoints[2 * i + 1] = normalized_y  # Store normalized y

                        # Add the frame's keypoints array to the keypoints list
                    self.keypoints_array.append(frame_keypoints)
                probs = result.probs  # Probs object for classification outputs
                obb = result.obb  # Oriented boxes object for OBB outputs
                # result.show()  # display to screen

                # result.save(filename="out_bdj_3.jpg")  # save to disk



            """å¤„ç†æ•°æ®========================================================================"""

            img = results.plot(line_width=1)    

            qImage = QtGui.QImage(img.data, img.shape[1], img.shape[0],
                                    QtGui.QImage.Format_RGB888)  # å˜æˆQImageå½¢å¼

            if self.stopFlag == False:
                self.label_treated.setPixmap(QtGui.QPixmap.fromImage(qImage))  # å¾€æ˜¾ç¤ºLabelé‡Œ æ˜¾ç¤ºQImage

            time.sleep(0.5)



    def stop(self, ):
        self.stopFlag = True      # è®© frameAnalyzeThreadFunc ä¸è¦å†è®¾ç½® label_treated
        self.timer_camera.stop()  # å…³é—­å®šæ—¶å™¨
        self.timer_videoFile.stop()  # å…³é—­å®šæ—¶å™¨

        if self.cap:
            self.cap.release()  # é‡Šæ”¾è§†é¢‘æµ

        # æ¸…ç©ºè§†é¢‘æ˜¾ç¤ºåŒºåŸŸ 
        self.label_ori_video.clear()        
        self.label_treated.clear()

        self.h = 0
        self.keypoints_array = []
        # # å»¶æ—¶500msæ¸…é™¤ï¼Œæœ‰çš„å®šæ—¶å™¨å¤„ç†ä»»åŠ¡å¯èƒ½ä¼šåœ¨å½“å‰æ—¶é—´ç‚¹åå¤„ç†å®Œæœ€åä¸€å¸§
        # QtCore.QTimer.singleShot(500, clearLabels)


    def startVideoFile(self):

        # å…ˆå…³é—­åŸæ¥æ‰“å¼€çš„
        self.stop()
        self.textLog.clear()
        videoPath, _  = QtWidgets.QFileDialog.getOpenFileName(
            self,             # çˆ¶çª—å£å¯¹è±¡
            "é€‰æ‹©è§†é¢‘æ–‡ä»¶",        # æ ‡é¢˜
            ".",               # èµ·å§‹ç›®å½•
            "å›¾ç‰‡ç±»å‹ (*.mp4 *.avi)" # é€‰æ‹©ç±»å‹è¿‡æ»¤é¡¹ï¼Œè¿‡æ»¤å†…å®¹åœ¨æ‹¬å·ä¸­
        )

        print('videoPath is', videoPath)
        if not videoPath:
            return


        self.cap = cv2.VideoCapture(videoPath)
        if not self.cap.isOpened():
            print("æ‰“å¼€æ–‡ä»¶å¤±è´¥")
            return


        self.timer_videoFile.start(30)
        self.stopFlag = False

        print("ok")


    def show_videoFile(self):
        # é€‰å–è§†é¢‘å¸§ä½ç½®ï¼Œ
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.vframeIdx)  
        self.vframeIdx += 1
        ret, frame = self.cap.read()  # ä»è§†é¢‘æµä¸­è¯»å–

        # è¯»å–å¤±è´¥ï¼Œåº”è¯¥æ˜¯è§†é¢‘æ’­æ”¾ç»“æŸäº†
        if not ret:
            self.keypoints_array = np.array(self.keypoints_array)
            # print(keypoints_array)
            if self.keypoints_array.size != 0:
                self.biaoji = self.keypoints_array
                self.textLog.setText(str('finall'))
            self.keypoints_array = []


            # self.stop()
            return
        # æŠŠè¯»åˆ°çš„å¸§çš„å¤§å°é‡æ–°è®¾ç½®
        frame = cv2.resize(frame, (520, 300))
        self.setFrameToOriLabel(frame)

    def sim(self, ):
        count = self.count
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei']  # è®¾ç½®ä¸ºé»‘ä½“
        plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

        # ç»˜åˆ¶å›¾å½¢
        plt.title("å•å¸§ç›¸ä¼¼åº¦å¯¹æ¯”ç»“æœ")
        plt.xlabel("è‚¢ä½“å‘é‡æ®µ")
        plt.ylabel("åˆ†æ•°")
        list = self.list[:-1]
        y = list / count
        self.count = 0
        self.list = []
        # ç¤ºä¾‹æ•°æ®
        x = ['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16']

        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        plt.bar(x, y)

        # ç»™å®šç›®å½•
        output_directory = r'D:\KK\bdjdatastes'  # æ›¿æ¢ä¸ºä½ å¸Œæœ›ä¿å­˜å›¾å½¢çš„æ–‡ä»¶å¤¹è·¯å¾„
        if not os.path.exists(output_directory):
            os.makedirs(output_directory)  # å¦‚æœç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º

        # ä¿å­˜å›¾åƒåˆ°æŒ‡å®šç›®å½•
        output_file = os.path.join(output_directory, 'similarity_comparison.png')  # è®¾ç½®æ–‡ä»¶åå’Œè·¯å¾„
        plt.savefig(output_file)

        print(f"å›¾åƒå·²ä¿å­˜åˆ° {output_file}")

        # æ˜¾ç¤ºå›¾å½¢
        plt.show()

        return 0
    def jisuan(self,):
        if self.biaoji.size !=0:
            fin_score, list, count = dtw_o_distance(self.biaoji)
            print(fin_score)
            self.textLog.setText(str(fin_score))
            self.count = count
            self.list = list
            self.biaoji = []
        return 0
app = QtWidgets.QApplication()
window = MWindow()
window.show()
app.exec()